# -*- coding: utf-8 -*-
"""单标签多分类问题之新闻主题分类.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18TqrbGYm2J-jmR89KZHOa7vxbAr4eOz2

### 问题解释

每个数据点只能划分到一个类别，但是类别总数大于2个，这类问题称之为单标签，多分类。如果每个数据点可以划分到多个类别，则就是另一个问题：多标签，多分类。

### 数据集

使用路透社数据集，其中包含很多短新闻和对应的主题，文本分类数据集。总共有46个不同的主题，每个主题至少有10个样本，有些主题样本会更多。

数据集是Keras内置的一部分，直接用即可。
"""

# 加载数据
from keras.datasets import reuters
(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000) # num_words=10000，将数据限定为前10000个最长出现的单词

train_data.shape

test_data.shape

train_data # 每个样本是一个list，表示单词索引

# 索引解码
word_index = reuters.get_word_index()

reverse_word_index = dict([(value, key) for (key, value) in word_index.items()]) # 键值翻转
decode_review = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[0]]) # 评论解码，索引去掉3，0为填充，1为序列开始，2位unknown

decode_review

train_labels[0]

# 数据准备
import numpy as np
def vectorize_sequences(sequences, dimension=10000):
  results = np.zeros((len(sequences), dimension))
  for i, sequence in enumerate(sequences):
    results[i, sequence] = 1
  return results

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

x_train.shape

"""### 向量化数据的解释

因为原始数据最外层是个一维数组，组成元素是list，我们需要的是二维数组形式的数据，所以需要把list拆出来，但是每个list长度不同，我们总共用10000个单词，基于one-hot编码，使得数据规整，列为10000即可。只有标签对应的元素存在才把对应的元素设定为1.

### 标签数据的向量化

和上面的原理相同。
"""

def to_one_hot(labels, dimension=46):
  results = np.zeros((len(labels), dimension))
  for i, label in enumerate(labels):
    results[i, label] = 1.
  return results

one_hot_train_labels = to_one_hot(train_labels)
one_hot_test_labels = to_one_hot(test_labels)

one_hot_train_labels[0].shape

# 上面是手工实现，其实也可以哟经Keras内置方法来做

# from keras.utils.np_utils import to_categorical
# one_hot_train_labels = to_categorical(train_labels)
# one_hot_test_labels = to_categorical(test_labels)

# 模型定义
from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64,activation='relu'))
model.add(layers.Dense(46, activation='softmax'))

"""### 模型解释

最后一层大小是46的Dense层，每个输入样本，网络都会输出一个46维的向量，每个元素代表不同的输出类别。而在最后一层使用的是softmax激活层，给出不同输出类别的概率值。

多分类问题最好的损失函数是categorical_crossentropy，即分类交叉熵函数。分类交叉熵函数可以用来衡量两个概率分布之间的距离，通过将两个分布的距离最小化，训练网络就能够将输出结果尽可能接近真实标签。
"""

# 编译模型，
# 配置损失函数和优化器

model.compile(optimizer='rmsprop',
             loss='categorical_crossentropy',
             metrics=['accuracy'])

# 留出验证集，从训练集中拿出来一部分做验证集，不是测试集
x_val = x_train[:1000]
partial_x_train = x_train[1000:]

y_val = one_hot_train_labels[:1000]
partial_y_train = one_hot_train_labels[1000:]

# 训练模型
history = model.fit(partial_x_train,partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))

history.history.keys()

# 绘制训练损失和验证损失
# 监控模型是否过拟合了

import matplotlib.pyplot as plt

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')

plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

"""### 分析

我们从训练集中抽出一部分来验证模型训练效果，算是训练过程里的自我监督。


从图中我们可以看到，大概在训练到10轮后，模型开始过拟合。所以我们可以选择在这个区间停止训练。
"""

# 绘制训练精度和验证精度
plt.clf()

acc = history.history['acc']
val_acc = history.history['val_acc']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo', label='Training loss')
plt.plot(epochs, val_acc, 'r', label='Validation loss')

plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

# 现在我们重新训练一个模型，只训练到第10轮

model2 = models.Sequential()
model2.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model2.add(layers.Dense(64, activation='relu'))
model2.add(layers.Dense(46, activation='softmax'))

model2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model2.fit(partial_x_train,
          partial_y_train,
          epochs=10,
          batch_size=512,
          validation_data=(x_val, y_val))

results = model.evaluate(x_test, one_hot_test_labels)

results

# 现在我们重新训练一个模型，只训练到第9轮

model3 = models.Sequential()
model3.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model3.add(layers.Dense(64, activation='relu'))
model3.add(layers.Dense(46, activation='softmax'))

model3.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model3.fit(partial_x_train,
          partial_y_train,
          epochs=9,
          batch_size=512,
          validation_data=(x_val, y_val))

results = model.evaluate(x_test, one_hot_test_labels)

results

# 使用模型预测结果

predictions = model2.predict(x_test)

predictions[0].argmax()

predictions[0][3]

"""### 实验总结

中间层的维度要足够大，因为信息的传递如果在当前层丢失了，那么后面就再也看不见了。所以隐藏层要足够大，比如最后输出是46维，中间层的神经元个数不能小于46，否则成了信息瓶颈，信息被压缩到小于输出空间，导致最后无法得出正确的结论。

### 知识点总结

- 对于N类分类问题，网络的最后一层大小是N的Dense层
- 单标签多分类问题，网络的最后一层用softmax激活函数，输出N个类别的概率分布
- 多分类问题的损失函数几乎都用分类交叉熵
- 处理多分类问题标签的两种方法：
  - one-hot编码，使用categorical_crossentropy作为损失函数
  - 编码为整数张量，使用sparse_categorical_crossentropy损失函数
"""

